{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2e6661e8",
   "metadata": {},
   "source": [
    "# Install dependencies in a virtual environment"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d2c2b061",
   "metadata": {},
   "source": [
    "Install packages for efficiently parsing CSV files."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb0b0e68",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install casanova"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "09059b10",
   "metadata": {},
   "source": [
    "Install packages for NLP analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9633ede0",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install bertopic hdbscan nltk sentence_transformers scikit-learn umap"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7165cb01",
   "metadata": {},
   "source": [
    "Also install `protobuf` version 3.20 because `SentenceTransformer` needs it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9030ebe6",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install protobuf==3.20.0"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d4e3f8c6",
   "metadata": {},
   "source": [
    "Install tools for visualizing the results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6104a56",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install networkx ipysigma plotly install pelote"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3eccb4de",
   "metadata": {},
   "source": [
    "# Prepare data\n",
    "\n",
    "We're going to be taking advantage of transformers and word-embedding, which in theory do not require pre-processing sentences that are sent to the topic model. Nevertheless, some pre-processing can be useful (i.e. removing HTML tags). In our case, we will want to remove the phrase \"Il faut\" at the start of every proposition because it is not meaningful in the context of our corpus."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "fe952b2c",
   "metadata": {},
   "outputs": [],
   "source": [
    "DATAFILE = \"mieux-sinformer.csv\" # change this according to file path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "90f639fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def special_preprocessing(string):\n",
    "    string = string.lower()\n",
    "    bow = string.split()\n",
    "    if bow[:2] == [\"il\", \"faut\"]:\n",
    "        bow = bow[2:]\n",
    "    return \" \".join(bow)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32916727",
   "metadata": {},
   "source": [
    "With that helper function in place, we'll parse our data file and create a list of our documents. (list of strings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "451f8f26",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset includes 1723 docs.\n"
     ]
    }
   ],
   "source": [
    "import casanova\n",
    "\n",
    "with open(DATAFILE) as f:\n",
    "    reader = casanova.reader(f)\n",
    "    docs = [special_preprocessing(cell) for cell in reader.cells(column=\"Proposition\")]\n",
    "    print(f\"Dataset includes {len(docs)} docs.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d499a96",
   "metadata": {},
   "source": [
    "We will also need to prepare a list of stop words, which will be excluded from the topics' representations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "40f2abbe",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus import stopwords\n",
    "\n",
    "stoplist = stopwords.words(\"french\")\n",
    "ADDITIONAL_STOPWORDS = [\"plus\", \"chaque\", \"tout\", \"tous\", \"toutes\", \"toute\", \"leur\", \"leurs\", \"comme\", \"afin\", \"pendant\", \"lorsque\"]\n",
    "stoplist.extend(ADDITIONAL_STOPWORDS)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98236674",
   "metadata": {},
   "source": [
    "# Vectorize the data\n",
    "\n",
    "### Word embeddings\n",
    "The first step is to transform a string (sentence) into an array of numbers (vector) or in other words, to \"vectorize\" the text. There are many ways to do this. By default, `BERTopic`'s privileges word embeddings, which are currently the most powerful way to encode a document because they represent words' relationships to other words in the context of a sentence.\n",
    "\n",
    "However, `BERTopic`'s default sentence transformer was trained on English-language texts. Because we are working with a corpus of sentences exclusively in French, we want to take advantage of a language model trained on French-language texts, such as the `CamemBERT` model. Fortunately, a data scientist at *La Javaness* Van Tuan Dang (\"dangvantuan\" on HuggingFace) has trained and published a sentence transformer model based on the `CamemBERT` base model. We will simply import this pre-trained transformer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "17030859",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "No sentence-transformers model found with name /Users/kelly.christensen/.cache/torch/sentence_transformers/dangvantuan_sentence-camembert-large. Creating a new one with MEAN pooling.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e7a370c8a60443f2a7be4da51419e529",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sentence 1 has 22 characters, and the embedding is 1024 long.\n",
      "Sentence 2 has 7 characters, and the embedding is 1024 long.\n"
     ]
    }
   ],
   "source": [
    "from sentence_transformers import SentenceTransformer\n",
    "\n",
    "sentences = ['Bonjour tout le monde.', 'Ça va ?']\n",
    "\n",
    "camembert_sentence_transformer = SentenceTransformer(\"dangvantuan/sentence-camembert-large\")\n",
    "example_embeddings = camembert_sentence_transformer.encode(sentences, show_progress_bar=True)\n",
    "\n",
    "print(f\"Sentence 1 has {len(sentences[0])} characters, and the embedding is {len(example_embeddings[0])} long.\")\n",
    "print(f\"Sentence 2 has {len(sentences[1])} characters, and the embedding is {len(example_embeddings[1])} long.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "736b35d2",
   "metadata": {},
   "source": [
    "As seen in the cell above, the embeddings created by Dang's sentence transformer `sentence-camembert-large` have the same length, despite the fact that the sentences the arrays represent are different lenghts. The embeddings have the same length because, rather than directly representing words as numbers (aka bag of words), the transformer creates a rich representation that takes into account 1024 dimensions."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f185f96",
   "metadata": {},
   "source": [
    "### Construct the topic model with the desired parameters\n",
    "\n",
    "Finally, having decided on an effective embedding model (very important!) and produced stop words for the topics' representations, we're ready to assemble the topic model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "90258ced",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sentence_transformers import SentenceTransformer\n",
    "from umap import UMAP\n",
    "from hdbscan import HDBSCAN\n",
    "from bertopic.vectorizers import ClassTfidfTransformer\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from bertopic import BERTopic\n",
    "\n",
    "stoplist = stopwords.words(\"french\")\n",
    "ADDITIONAL_STOPWORDS = [\"plus\", \"chaque\", \"tout\", \"tous\", \"toutes\", \"toute\", \"leur\", \"leurs\", \"comme\", \"afin\", \"pendant\", \"lorsque\"]\n",
    "\n",
    "def set_model_parameters(embedding_model):\n",
    "\n",
    "    # Step 1 - Extract embeddings\n",
    "    embedding_model = embedding_model\n",
    "\n",
    "    # Step 2 - Reduce dimensionality\n",
    "    umap_model = UMAP(angular_rp_forest=True, metric='cosine', n_components=10, n_neighbors=30, min_dist=0.1)\n",
    "\n",
    "    # Step 3 - Cluster reduced embeddings\n",
    "    hdbscan_model = HDBSCAN(min_cluster_size=13, min_samples=3, prediction_data=True, metric='euclidean', cluster_selection_method='eom')\n",
    "\n",
    "    # Step 4 - Tokenize topics\n",
    "    stoplist.extend(ADDITIONAL_STOPWORDS)\n",
    "    vectorizer_model = CountVectorizer(stop_words=stoplist, ngram_range=(1, 3))\n",
    "\n",
    "    # Step 5 - Create topic representation\n",
    "    ctfidf_model = ClassTfidfTransformer(reduce_frequent_words=True, bm25_weighting=True)\n",
    "\n",
    "    # Topic model\n",
    "    return BERTopic(\n",
    "        embedding_model=embedding_model,\n",
    "        umap_model=umap_model,\n",
    "        hdbscan_model=hdbscan_model,\n",
    "        vectorizer_model=vectorizer_model,\n",
    "        ctfidf_model=ctfidf_model,\n",
    "        diversity=0.5,\n",
    "        n_gram_range=(1,3),\n",
    "        nr_topics='auto',\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf00dd83",
   "metadata": {},
   "source": [
    "# Fit the topic model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c2be5a1",
   "metadata": {},
   "source": [
    "Download the pre-trained embedding model from `SentenceTransformers` and attribute it to the variable `embedding model`. Then use that model to encode all the documents in our corpus."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "bb4b5c7a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "No sentence-transformers model found with name /Users/kelly.christensen/.cache/torch/sentence_transformers/dangvantuan_sentence-camembert-large. Creating a new one with MEAN pooling.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bdc031ff92da4fba9bbf08eacab94278",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/54 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from sentence_transformers import SentenceTransformer\n",
    "\n",
    "embedding_model = SentenceTransformer(\"dangvantuan/sentence-camembert-large\")\n",
    "embeddings = camembert_sentence_transformer.encode(docs, show_progress_bar=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "75196a5c",
   "metadata": {},
   "source": [
    "Create an instance of the model with all the parameters defined and our selected embedding model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "0d605d27",
   "metadata": {},
   "outputs": [],
   "source": [
    "topic_model = set_model_parameters(embedding_model)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "546f4816",
   "metadata": {},
   "source": [
    "Fit the model to the documents in our corpus and to the embeddings we prepared."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "773525a2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Topic</th>\n",
       "      <th>Count</th>\n",
       "      <th>Name</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>-1</td>\n",
       "      <td>465</td>\n",
       "      <td>-1_sans_citoyens_médias_donner</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>325</td>\n",
       "      <td>0_esprit_experts_information_politiques</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>249</td>\n",
       "      <td>1_financement_concentration médias_milliardair...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2</td>\n",
       "      <td>226</td>\n",
       "      <td>2_fake news_france_chaînes_tv</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>3</td>\n",
       "      <td>112</td>\n",
       "      <td>3_algorithmes_facebook_twitter_identité</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>4</td>\n",
       "      <td>81</td>\n",
       "      <td>4_médias information_information_enseignement_...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>5</td>\n",
       "      <td>57</td>\n",
       "      <td>5_journaux_rendre accessible_papier_netflix</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>6</td>\n",
       "      <td>46</td>\n",
       "      <td>6_charte munich_déontologie journalistique_pro...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>7</td>\n",
       "      <td>40</td>\n",
       "      <td>7_sensibiliser_réseaux sociaux former_images_i...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>8</td>\n",
       "      <td>38</td>\n",
       "      <td>8_arrêtent_journalistes restent_fact checking ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>9</td>\n",
       "      <td>21</td>\n",
       "      <td>9_dès école maternelle_sociologie_permettre en...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>10</td>\n",
       "      <td>19</td>\n",
       "      <td>10_formation scientifique_politiques journalis...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>11</td>\n",
       "      <td>15</td>\n",
       "      <td>11_presse étrangère_traduits_différent_point vue</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>12</td>\n",
       "      <td>15</td>\n",
       "      <td>12_emploi temps élèves_élèves_enseignement emi...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>13</td>\n",
       "      <td>14</td>\n",
       "      <td>13_sanctionner youtube lorsqu_médias informati...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    Topic  Count                                               Name\n",
       "0      -1    465                     -1_sans_citoyens_médias_donner\n",
       "1       0    325            0_esprit_experts_information_politiques\n",
       "2       1    249  1_financement_concentration médias_milliardair...\n",
       "3       2    226                      2_fake news_france_chaînes_tv\n",
       "4       3    112            3_algorithmes_facebook_twitter_identité\n",
       "5       4     81  4_médias information_information_enseignement_...\n",
       "6       5     57        5_journaux_rendre accessible_papier_netflix\n",
       "7       6     46  6_charte munich_déontologie journalistique_pro...\n",
       "8       7     40  7_sensibiliser_réseaux sociaux former_images_i...\n",
       "9       8     38  8_arrêtent_journalistes restent_fact checking ...\n",
       "10      9     21  9_dès école maternelle_sociologie_permettre en...\n",
       "11     10     19  10_formation scientifique_politiques journalis...\n",
       "12     11     15   11_presse étrangère_traduits_différent_point vue\n",
       "13     12     15  12_emploi temps élèves_élèves_enseignement emi...\n",
       "14     13     14  13_sanctionner youtube lorsqu_médias informati..."
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "topic_model.fit(docs, embeddings)\n",
    "\n",
    "topic_model.get_topic_info()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b9c10732",
   "metadata": {},
   "source": [
    "### Explore the model's predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "c4a28bde",
   "metadata": {},
   "outputs": [],
   "source": [
    "# optional\n",
    "topic_model = BERTopic.load('best_bertopic.model')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "8d163e65",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████| 14/14 [00:06<00:00,  2.14it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ".\n",
      "├─anonymat réseaux sociaux_identité_réseaux sociaux responsables_supprimer_diffusent\n",
      "│    ├─■──réseaux sociaux responsables_diffusent_détenteurs_responsables contenus_sociaux considérés ── Topic: 12\n",
      "│    └─■──anonymat réseaux sociaux_réseaux sociaux interdire_supprimer_vrai nom_bannir ── Topic: 10\n",
      "└─journalistes_éducation médias_école_news_fake\n",
      "     ├─partiaux_orientation politique_tendance_neutralité_lfi\n",
      "     │    ├─orientation politique_tendance_neutralité_affiliations_lfi\n",
      "     │    │    ├─■──laissent_journalistes restent_humilité_neutres_propagande ── Topic: 9\n",
      "     │    │    └─■──gauche_lfi_orienté_partiaux_politique information ── Topic: 7\n",
      "     │    └─■──laisser liberté_liberté presse_influence_gouvernement_médias annoncent mouvance ── Topic: 11\n",
      "     └─presse_éducation médias_école_créer_fake news\n",
      "          ├─éducation médias_école_presse_public_fake news\n",
      "          │    ├─■──charte munich_créer_conseil_code déontologie_déontologie médias ── Topic: 6\n",
      "          │    └─éducation médias_jeunes_école_presse_public\n",
      "          │         ├─jeunes_école_fake_news_esprit critique\n",
      "          │         │    ├─jeunes_école_news_fake_esprit critique\n",
      "          │         │    │    ├─■──école_jeunes_esprit critique_professeurs documentalistes_éducation médias information ── Topic: 0\n",
      "          │         │    │    └─fake news_algorithmes_france_chaînes_tv\n",
      "          │         │    │         ├─■──sanctionner_pubs_arnaques_algorithmes réseaux sociaux_modération ── Topic: 2\n",
      "          │         │    │         └─■──chaînes_arrêter_buzz_jt_tv ── Topic: 3\n",
      "          │         │    └─■──sources_véracité_obligatoirement_indépendant_créer label ── Topic: 4\n",
      "          │         └─financement_milliardaires_concentration médias_journaux_grands groupes\n",
      "          │              ├─■──financement_concentration médias_milliardaires_indépendants_interdire ── Topic: 1\n",
      "          │              └─■──papier_journaux_rendre accessible_netflix_abonnements gratuits ── Topic: 5\n",
      "          └─invités_étrangère_diversifier sources information_points vue_articles presse\n",
      "               ├─presse étrangère_diversifier sources information_points vue_plusieurs_lecture\n",
      "               │    ├─■──mieux informer_monde compris adultes_multiple diverse_lire beaucoup comprendre_lire presse internati ── Topic: 14\n",
      "               │    └─■──presse étrangère_plusieurs sources_diversifier sources information_plusieurs_quand parle sujet ── Topic: 8\n",
      "               └─■──intellectuels_donner parole experts_sujet_vrais_non utiliser personnes ── Topic: 13\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "hierarchical_topics = topic_model.hierarchical_topics(docs)\n",
    "\n",
    "tree = topic_model.get_topic_tree(hierarchical_topics)\n",
    "print(tree)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "d9a4d712",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<iframe\n",
       "    scrolling=\"no\"\n",
       "    width=\"1020px\"\n",
       "    height=\"445\"\n",
       "    src=\"iframe_figures/figure_22.html\"\n",
       "    frameborder=\"0\"\n",
       "    allowfullscreen\n",
       "></iframe>\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "topic_model.visualize_hierarchy(hierarchical_topics=hierarchical_topics).show(renderer='iframe_connected')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "76eab258",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<iframe\n",
       "    scrolling=\"no\"\n",
       "    width=\"1020px\"\n",
       "    height=\"520\"\n",
       "    src=\"iframe_figures/figure_23.html\"\n",
       "    frameborder=\"0\"\n",
       "    allowfullscreen\n",
       "></iframe>\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "topic_model.visualize_barchart().show(renderer='iframe_connected')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "949ace67",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<iframe\n",
       "    scrolling=\"no\"\n",
       "    width=\"820px\"\n",
       "    height=\"820\"\n",
       "    src=\"iframe_figures/figure_24.html\"\n",
       "    frameborder=\"0\"\n",
       "    allowfullscreen\n",
       "></iframe>\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "topic_model.visualize_heatmap(n_clusters=4).show(renderer='iframe_connected')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "78bf41be",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<iframe\n",
       "    scrolling=\"no\"\n",
       "    width=\"820px\"\n",
       "    height=\"520\"\n",
       "    src=\"iframe_figures/figure_25.html\"\n",
       "    frameborder=\"0\"\n",
       "    allowfullscreen\n",
       "></iframe>\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "topic_model.visualize_term_rank(log_scale=True).show(renderer='iframe_connected')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "efde642d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<iframe\n",
       "    scrolling=\"no\"\n",
       "    width=\"670px\"\n",
       "    height=\"670\"\n",
       "    src=\"iframe_figures/figure_26.html\"\n",
       "    frameborder=\"0\"\n",
       "    allowfullscreen\n",
       "></iframe>\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "topic_model.visualize_topics().show(renderer='iframe_connected')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "ab85f6cd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<iframe\n",
       "    scrolling=\"no\"\n",
       "    width=\"1220px\"\n",
       "    height=\"770\"\n",
       "    src=\"iframe_figures/figure_27.html\"\n",
       "    frameborder=\"0\"\n",
       "    allowfullscreen\n",
       "></iframe>\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "reduced_embeddings = UMAP(n_neighbors=10, n_components=2, min_dist=0.0, metric='cosine').fit_transform(embeddings)\n",
    "topic_model.visualize_documents(docs, reduced_embeddings=reduced_embeddings).show(renderer='iframe_connected')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24baae50",
   "metadata": {},
   "source": [
    "Update the topic labels."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "4840c414",
   "metadata": {},
   "outputs": [],
   "source": [
    "topic_labels = topic_model.generate_topic_labels(nr_words=3,\n",
    "                                topic_prefix=True,\n",
    "                                separator=\" - \")\n",
    "\n",
    "topic_model.set_topic_labels(topic_labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d0f5b55",
   "metadata": {},
   "source": [
    "Print the results to a CSV file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "689716ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "import casanova\n",
    "\n",
    "PREDICTIONS_FILE = 'bertopic_topics_notebook.csv'\n",
    "\n",
    "results = topic_model.get_document_info(docs=docs)\n",
    "results.to_csv()\n",
    "with open(DATAFILE) as f, open(PREDICTIONS_FILE, 'w') as of:\n",
    "    enricher = casanova.enricher(f, of, add=[\"document\", \"topic\", \"name\", \"top_n_words\", \"probability\", \"representative_document\"])\n",
    "    for i, row in enumerate(enricher):\n",
    "        enricher.writerow(row=row, add=[results[\"Document\"][i], results[\"Topic\"][i], results[\"Name\"][i], results[\"Top_n_words\"][i], results[\"Probability\"][i], results[\"Representative_document\"][i]])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb76ecc1",
   "metadata": {},
   "source": [
    "# Visualize the results"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9463d5c1",
   "metadata": {},
   "source": [
    "In a dictionary, index the documents and their metadata by each document's Id."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "3e7ccda1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "\n",
    "with open(PREDICTIONS_FILE) as f:\n",
    "    reader = csv.DictReader(f)\n",
    "    for row in reader:\n",
    "        doc_index = {row[\"Id\"]:row for row in reader}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "65fbeba9",
   "metadata": {},
   "source": [
    "While parsing the matrix file, create nodes and edges and label them with data from the topic predictions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "b4efd96d",
   "metadata": {},
   "outputs": [],
   "source": [
    "MATRIX_FILE = 'defacto_covotes.csv'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "70f74086",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "IOPub data rate exceeded.\n",
      "The notebook server will temporarily stop sending output\n",
      "to the client in order to avoid crashing it.\n",
      "To change this limit, set the config variable\n",
      "`--NotebookApp.iopub_data_rate_limit`.\n",
      "\n",
      "Current values:\n",
      "NotebookApp.iopub_data_rate_limit=1000000.0 (bytes/sec)\n",
      "NotebookApp.rate_limit_window=3.0 (secs)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import networkx as nx\n",
    "from statistics import mean\n",
    "import casanova\n",
    "\n",
    "with open(MATRIX_FILE) as f:\n",
    "    reader = casanova.reader(f)\n",
    "    pid1_pos = reader.headers['pid1']\n",
    "    pid2_pos = reader.headers['pid2']\n",
    "    vote1_pos = reader.headers['vote1']\n",
    "    vote2_pos = reader.headers['vote2']\n",
    "    count_pos = reader.headers['count']\n",
    "    \n",
    "    G = nx.Graph()\n",
    "    for row in reader:\n",
    "        pid1 = row[pid1_pos]\n",
    "        pid2 = row[pid2_pos]\n",
    "\n",
    "        # If the matrix refers to documents not in the original data, skip \n",
    "        if not doc_index.get(pid1) or not doc_index.get(pid2):\n",
    "            continue\n",
    "\n",
    "        # Base an edge's weight on the support that two related propositions received\n",
    "        average_nb_votes = mean([int(doc_index[pid1][\"Nb de votes\"]), int(doc_index[pid2][\"Nb de votes\"])])\n",
    "        weight = int(row[count_pos])/average_nb_votes\n",
    "\n",
    "        # Unless already added to the Graph, add both nodes in the matrix row and create an edge between them\n",
    "        print(doc_index[pid1])\n",
    "        if not G.has_node(pid1) and not str(doc_index[pid1][\"topic\"]) == \"-1\":\n",
    "            G.add_node(pid1, label=doc_index[pid1][\"Proposition\"], **doc_index[pid1])\n",
    "\n",
    "        if not G.has_node(pid2) and not str(doc_index[pid2][\"topic\"]) == \"-1\":\n",
    "            G.add_node(pid2, label=doc_index[pid2][\"Proposition\"], **doc_index[pid2])\n",
    "\n",
    "        if row[vote1_pos] == row[vote2_pos]:  # To-do: test if we can try all 3 types of cases\n",
    "            G.add_edge(pid1, pid2, weight=weight)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "4bcd266e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pelote\n",
    "\n",
    "GEFX_FILE = 'sparsification.gexf'\n",
    "\n",
    "H = pelote.multiscale_backbone(G, alpha=0.05)\n",
    "nx.write_gexf(H, GEFX_FILE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "0319d78c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import networkx as nx\n",
    "from ipysigma import Sigma"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "c5f3e7af",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importing a gexf graph\n",
    "g = nx.read_gexf(GEFX_FILE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "ad6bc506",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0a33512526f54b0da6ce2e7b1bf3f415",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Sigma(nx.Graph with 1,722 nodes and 30,552 edges)"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Displaying the graph with a size mapped on degree and\n",
    "# a color mapped on a categorical attribute of the nodes\n",
    "Sigma(g, node_size=g.degree, node_color='topic')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54e0445d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
