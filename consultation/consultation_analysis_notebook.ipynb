{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2e6661e8",
   "metadata": {},
   "source": [
    "# Install dependencies in a virtual environment"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d2c2b061",
   "metadata": {},
   "source": [
    "Install packages for efficiently parsing CSV files."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb0b0e68",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install casanova"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "09059b10",
   "metadata": {},
   "source": [
    "Install packages for NLP analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9633ede0",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install bertopic hdbscan nltk sentence_transformers scikit-learn umap"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7165cb01",
   "metadata": {},
   "source": [
    "Also install `protobuf` version 3.20 because `SentenceTransformer` needs it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9030ebe6",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install protobuf==3.20.0"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d4e3f8c6",
   "metadata": {},
   "source": [
    "Install tools for visualizing the results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6104a56",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install networkx ipysigma plotly install pelote"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3eccb4de",
   "metadata": {},
   "source": [
    "# Prepare data\n",
    "\n",
    "We're going to be taking advantage of transformers and word-embedding, which in theory do not require pre-processing sentences that are sent to the topic model. Nevertheless, some pre-processing can be useful (i.e. removing HTML tags). In our case, we will want to remove the phrase \"Il faut\" at the start of every proposition because it is not meaningful in the context of our corpus."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "fe952b2c",
   "metadata": {},
   "outputs": [],
   "source": [
    "DATAFILE = \"mieux-sinformer.csv\" # change this according to file path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "90f639fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def special_preprocessing(string):\n",
    "    string = string.lower()\n",
    "    bow = string.split()\n",
    "    if bow[:2] == [\"il\", \"faut\"]:\n",
    "        bow = bow[2:]\n",
    "    return \" \".join(bow)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32916727",
   "metadata": {},
   "source": [
    "With that helper function in place, we'll parse our data file and create a list of our documents. (list of strings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "451f8f26",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset includes 1723 docs.\n"
     ]
    }
   ],
   "source": [
    "import casanova\n",
    "\n",
    "with open(DATAFILE) as f:\n",
    "    reader = casanova.reader(f)\n",
    "    docs = [special_preprocessing(cell) for cell in reader.cells(column=\"Proposition\")]\n",
    "    print(f\"Dataset includes {len(docs)} docs.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d499a96",
   "metadata": {},
   "source": [
    "We will also need to prepare a list of stop words, which will be excluded from the topics' representations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "40f2abbe",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus import stopwords\n",
    "\n",
    "stoplist = stopwords.words(\"french\")\n",
    "ADDITIONAL_STOPWORDS = [\"plus\", \"chaque\", \"tout\", \"tous\", \"toutes\", \"toute\", \"leur\", \"leurs\", \"comme\", \"afin\", \"pendant\", \"lorsque\"]\n",
    "stoplist.extend(ADDITIONAL_STOPWORDS)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98236674",
   "metadata": {},
   "source": [
    "# Vectorize the data\n",
    "\n",
    "### Word embeddings\n",
    "The first step is to transform a string (sentence) into an array of numbers (vector) or in other words, to \"vectorize\" the text. There are many ways to do this. By default, `BERTopic`'s privileges word embeddings, which are currently the most powerful way to encode a document because they represent words' relationships to other words in the context of a sentence.\n",
    "\n",
    "However, `BERTopic`'s default sentence transformer was trained on English-language texts. Because we are working with a corpus of sentences exclusively in French, we want to take advantage of a language model trained on French-language texts, such as the `CamemBERT` model. Fortunately, a data scientist at *La Javaness* Van Tuan Dang (\"dangvantuan\" on HuggingFace) has trained and published a sentence transformer model based on the `CamemBERT` base model. We will simply import this pre-trained transformer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "17030859",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "No sentence-transformers model found with name /Users/kelly.christensen/.cache/torch/sentence_transformers/dangvantuan_sentence-camembert-large. Creating a new one with MEAN pooling.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e7a370c8a60443f2a7be4da51419e529",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sentence 1 has 22 characters, and the embedding is 1024 long.\n",
      "Sentence 2 has 7 characters, and the embedding is 1024 long.\n"
     ]
    }
   ],
   "source": [
    "from sentence_transformers import SentenceTransformer\n",
    "\n",
    "sentences = ['Bonjour tout le monde.', 'Ã‡a va ?']\n",
    "\n",
    "camembert_sentence_transformer = SentenceTransformer(\"dangvantuan/sentence-camembert-large\")\n",
    "example_embeddings = camembert_sentence_transformer.encode(sentences, show_progress_bar=True)\n",
    "\n",
    "print(f\"Sentence 1 has {len(sentences[0])} characters, and the embedding is {len(example_embeddings[0])} long.\")\n",
    "print(f\"Sentence 2 has {len(sentences[1])} characters, and the embedding is {len(example_embeddings[1])} long.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "736b35d2",
   "metadata": {},
   "source": [
    "As seen in the cell above, the embeddings created by Dang's sentence transformer `sentence-camembert-large` have the same length, despite the fact that the sentences the arrays represent are different lenghts. The embeddings have the same length because, rather than directly representing words as numbers (aka bag of words), the transformer creates a rich representation that takes into account 1024 dimensions."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f185f96",
   "metadata": {},
   "source": [
    "### Construct the topic model with the desired parameters\n",
    "\n",
    "Finally, having decided on an effective embedding model (very important!) and produced stop words for the topics' representations, we're ready to assemble the topic model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "90258ced",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sentence_transformers import SentenceTransformer\n",
    "from umap import UMAP\n",
    "from hdbscan import HDBSCAN\n",
    "from bertopic.vectorizers import ClassTfidfTransformer\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from bertopic import BERTopic\n",
    "\n",
    "stoplist = stopwords.words(\"french\")\n",
    "ADDITIONAL_STOPWORDS = [\"plus\", \"chaque\", \"tout\", \"tous\", \"toutes\", \"toute\", \"leur\", \"leurs\", \"comme\", \"afin\", \"pendant\", \"lorsque\"]\n",
    "\n",
    "def set_model_parameters(embedding_model):\n",
    "\n",
    "    # Step 1 - Extract embeddings\n",
    "    embedding_model = embedding_model\n",
    "\n",
    "    # Step 2 - Reduce dimensionality\n",
    "    umap_model = UMAP(angular_rp_forest=True, metric='cosine', n_components=10, n_neighbors=30, min_dist=0.1)\n",
    "\n",
    "    # Step 3 - Cluster reduced embeddings\n",
    "    hdbscan_model = HDBSCAN(min_cluster_size=13, min_samples=3, prediction_data=True, metric='euclidean', cluster_selection_method='eom')\n",
    "\n",
    "    # Step 4 - Tokenize topics\n",
    "    stoplist.extend(ADDITIONAL_STOPWORDS)\n",
    "    vectorizer_model = CountVectorizer(stop_words=stoplist, ngram_range=(1, 3))\n",
    "\n",
    "    # Step 5 - Create topic representation\n",
    "    ctfidf_model = ClassTfidfTransformer(reduce_frequent_words=True, bm25_weighting=True)\n",
    "\n",
    "    # Topic model\n",
    "    return BERTopic(\n",
    "        embedding_model=embedding_model,\n",
    "        umap_model=umap_model,\n",
    "        hdbscan_model=hdbscan_model,\n",
    "        vectorizer_model=vectorizer_model,\n",
    "        ctfidf_model=ctfidf_model,\n",
    "        diversity=0.5,\n",
    "        n_gram_range=(1,3),\n",
    "        nr_topics='auto',\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf00dd83",
   "metadata": {},
   "source": [
    "# Fit the topic model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c2be5a1",
   "metadata": {},
   "source": [
    "Download the pre-trained embedding model from `SentenceTransformers` and attribute it to the variable `embedding model`. Then use that model to encode all the documents in our corpus."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "bb4b5c7a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "No sentence-transformers model found with name /Users/kelly.christensen/.cache/torch/sentence_transformers/dangvantuan_sentence-camembert-large. Creating a new one with MEAN pooling.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bdc031ff92da4fba9bbf08eacab94278",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/54 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from sentence_transformers import SentenceTransformer\n",
    "\n",
    "embedding_model = SentenceTransformer(\"dangvantuan/sentence-camembert-large\")\n",
    "embeddings = camembert_sentence_transformer.encode(docs, show_progress_bar=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "75196a5c",
   "metadata": {},
   "source": [
    "Create an instance of the model with all the parameters defined and our selected embedding model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "0d605d27",
   "metadata": {},
   "outputs": [],
   "source": [
    "topic_model = set_model_parameters(embedding_model)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "546f4816",
   "metadata": {},
   "source": [
    "Fit the model to the documents in our corpus and to the embeddings we prepared."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "773525a2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Topic</th>\n",
       "      <th>Count</th>\n",
       "      <th>Name</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>-1</td>\n",
       "      <td>465</td>\n",
       "      <td>-1_sans_citoyens_mÃ©dias_donner</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>325</td>\n",
       "      <td>0_esprit_experts_information_politiques</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>249</td>\n",
       "      <td>1_financement_concentration mÃ©dias_milliardair...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2</td>\n",
       "      <td>226</td>\n",
       "      <td>2_fake news_france_chaÃ®nes_tv</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>3</td>\n",
       "      <td>112</td>\n",
       "      <td>3_algorithmes_facebook_twitter_identitÃ©</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>4</td>\n",
       "      <td>81</td>\n",
       "      <td>4_mÃ©dias information_information_enseignement_...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>5</td>\n",
       "      <td>57</td>\n",
       "      <td>5_journaux_rendre accessible_papier_netflix</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>6</td>\n",
       "      <td>46</td>\n",
       "      <td>6_charte munich_dÃ©ontologie journalistique_pro...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>7</td>\n",
       "      <td>40</td>\n",
       "      <td>7_sensibiliser_rÃ©seaux sociaux former_images_i...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>8</td>\n",
       "      <td>38</td>\n",
       "      <td>8_arrÃªtent_journalistes restent_fact checking ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>9</td>\n",
       "      <td>21</td>\n",
       "      <td>9_dÃ¨s Ã©cole maternelle_sociologie_permettre en...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>10</td>\n",
       "      <td>19</td>\n",
       "      <td>10_formation scientifique_politiques journalis...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>11</td>\n",
       "      <td>15</td>\n",
       "      <td>11_presse Ã©trangÃ¨re_traduits_diffÃ©rent_point vue</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>12</td>\n",
       "      <td>15</td>\n",
       "      <td>12_emploi temps Ã©lÃ¨ves_Ã©lÃ¨ves_enseignement emi...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>13</td>\n",
       "      <td>14</td>\n",
       "      <td>13_sanctionner youtube lorsqu_mÃ©dias informati...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    Topic  Count                                               Name\n",
       "0      -1    465                     -1_sans_citoyens_mÃ©dias_donner\n",
       "1       0    325            0_esprit_experts_information_politiques\n",
       "2       1    249  1_financement_concentration mÃ©dias_milliardair...\n",
       "3       2    226                      2_fake news_france_chaÃ®nes_tv\n",
       "4       3    112            3_algorithmes_facebook_twitter_identitÃ©\n",
       "5       4     81  4_mÃ©dias information_information_enseignement_...\n",
       "6       5     57        5_journaux_rendre accessible_papier_netflix\n",
       "7       6     46  6_charte munich_dÃ©ontologie journalistique_pro...\n",
       "8       7     40  7_sensibiliser_rÃ©seaux sociaux former_images_i...\n",
       "9       8     38  8_arrÃªtent_journalistes restent_fact checking ...\n",
       "10      9     21  9_dÃ¨s Ã©cole maternelle_sociologie_permettre en...\n",
       "11     10     19  10_formation scientifique_politiques journalis...\n",
       "12     11     15   11_presse Ã©trangÃ¨re_traduits_diffÃ©rent_point vue\n",
       "13     12     15  12_emploi temps Ã©lÃ¨ves_Ã©lÃ¨ves_enseignement emi...\n",
       "14     13     14  13_sanctionner youtube lorsqu_mÃ©dias informati..."
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "topic_model.fit(docs, embeddings)\n",
    "\n",
    "topic_model.get_topic_info()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b9c10732",
   "metadata": {},
   "source": [
    "### Explore the model's predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "c4a28bde",
   "metadata": {},
   "outputs": [],
   "source": [
    "# optional\n",
    "topic_model = BERTopic.load('best_bertopic.model')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "8d163e65",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 14/14 [00:06<00:00,  2.14it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ".\n",
      "â”œâ”€anonymat rÃ©seaux sociaux_identitÃ©_rÃ©seaux sociaux responsables_supprimer_diffusent\n",
      "â”‚    â”œâ”€â– â”€â”€rÃ©seaux sociaux responsables_diffusent_dÃ©tenteurs_responsables contenus_sociaux considÃ©rÃ©s â”€â”€ Topic: 12\n",
      "â”‚    â””â”€â– â”€â”€anonymat rÃ©seaux sociaux_rÃ©seaux sociaux interdire_supprimer_vrai nom_bannir â”€â”€ Topic: 10\n",
      "â””â”€journalistes_Ã©ducation mÃ©dias_Ã©cole_news_fake\n",
      "     â”œâ”€partiaux_orientation politique_tendance_neutralitÃ©_lfi\n",
      "     â”‚    â”œâ”€orientation politique_tendance_neutralitÃ©_affiliations_lfi\n",
      "     â”‚    â”‚    â”œâ”€â– â”€â”€laissent_journalistes restent_humilitÃ©_neutres_propagande â”€â”€ Topic: 9\n",
      "     â”‚    â”‚    â””â”€â– â”€â”€gauche_lfi_orientÃ©_partiaux_politique information â”€â”€ Topic: 7\n",
      "     â”‚    â””â”€â– â”€â”€laisser libertÃ©_libertÃ© presse_influence_gouvernement_mÃ©dias annoncent mouvance â”€â”€ Topic: 11\n",
      "     â””â”€presse_Ã©ducation mÃ©dias_Ã©cole_crÃ©er_fake news\n",
      "          â”œâ”€Ã©ducation mÃ©dias_Ã©cole_presse_public_fake news\n",
      "          â”‚    â”œâ”€â– â”€â”€charte munich_crÃ©er_conseil_code dÃ©ontologie_dÃ©ontologie mÃ©dias â”€â”€ Topic: 6\n",
      "          â”‚    â””â”€Ã©ducation mÃ©dias_jeunes_Ã©cole_presse_public\n",
      "          â”‚         â”œâ”€jeunes_Ã©cole_fake_news_esprit critique\n",
      "          â”‚         â”‚    â”œâ”€jeunes_Ã©cole_news_fake_esprit critique\n",
      "          â”‚         â”‚    â”‚    â”œâ”€â– â”€â”€Ã©cole_jeunes_esprit critique_professeurs documentalistes_Ã©ducation mÃ©dias information â”€â”€ Topic: 0\n",
      "          â”‚         â”‚    â”‚    â””â”€fake news_algorithmes_france_chaÃ®nes_tv\n",
      "          â”‚         â”‚    â”‚         â”œâ”€â– â”€â”€sanctionner_pubs_arnaques_algorithmes rÃ©seaux sociaux_modÃ©ration â”€â”€ Topic: 2\n",
      "          â”‚         â”‚    â”‚         â””â”€â– â”€â”€chaÃ®nes_arrÃªter_buzz_jt_tv â”€â”€ Topic: 3\n",
      "          â”‚         â”‚    â””â”€â– â”€â”€sources_vÃ©racitÃ©_obligatoirement_indÃ©pendant_crÃ©er label â”€â”€ Topic: 4\n",
      "          â”‚         â””â”€financement_milliardaires_concentration mÃ©dias_journaux_grands groupes\n",
      "          â”‚              â”œâ”€â– â”€â”€financement_concentration mÃ©dias_milliardaires_indÃ©pendants_interdire â”€â”€ Topic: 1\n",
      "          â”‚              â””â”€â– â”€â”€papier_journaux_rendre accessible_netflix_abonnements gratuits â”€â”€ Topic: 5\n",
      "          â””â”€invitÃ©s_Ã©trangÃ¨re_diversifier sources information_points vue_articles presse\n",
      "               â”œâ”€presse Ã©trangÃ¨re_diversifier sources information_points vue_plusieurs_lecture\n",
      "               â”‚    â”œâ”€â– â”€â”€mieux informer_monde compris adultes_multiple diverse_lire beaucoup comprendre_lire presse internati â”€â”€ Topic: 14\n",
      "               â”‚    â””â”€â– â”€â”€presse Ã©trangÃ¨re_plusieurs sources_diversifier sources information_plusieurs_quand parle sujet â”€â”€ Topic: 8\n",
      "               â””â”€â– â”€â”€intellectuels_donner parole experts_sujet_vrais_non utiliser personnes â”€â”€ Topic: 13\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "hierarchical_topics = topic_model.hierarchical_topics(docs)\n",
    "\n",
    "tree = topic_model.get_topic_tree(hierarchical_topics)\n",
    "print(tree)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "d9a4d712",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<iframe\n",
       "    scrolling=\"no\"\n",
       "    width=\"1020px\"\n",
       "    height=\"445\"\n",
       "    src=\"iframe_figures/figure_22.html\"\n",
       "    frameborder=\"0\"\n",
       "    allowfullscreen\n",
       "></iframe>\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "topic_model.visualize_hierarchy(hierarchical_topics=hierarchical_topics).show(renderer='iframe_connected')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "76eab258",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<iframe\n",
       "    scrolling=\"no\"\n",
       "    width=\"1020px\"\n",
       "    height=\"520\"\n",
       "    src=\"iframe_figures/figure_23.html\"\n",
       "    frameborder=\"0\"\n",
       "    allowfullscreen\n",
       "></iframe>\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "topic_model.visualize_barchart().show(renderer='iframe_connected')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "949ace67",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<iframe\n",
       "    scrolling=\"no\"\n",
       "    width=\"820px\"\n",
       "    height=\"820\"\n",
       "    src=\"iframe_figures/figure_24.html\"\n",
       "    frameborder=\"0\"\n",
       "    allowfullscreen\n",
       "></iframe>\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "topic_model.visualize_heatmap(n_clusters=4).show(renderer='iframe_connected')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "78bf41be",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<iframe\n",
       "    scrolling=\"no\"\n",
       "    width=\"820px\"\n",
       "    height=\"520\"\n",
       "    src=\"iframe_figures/figure_25.html\"\n",
       "    frameborder=\"0\"\n",
       "    allowfullscreen\n",
       "></iframe>\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "topic_model.visualize_term_rank(log_scale=True).show(renderer='iframe_connected')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "efde642d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<iframe\n",
       "    scrolling=\"no\"\n",
       "    width=\"670px\"\n",
       "    height=\"670\"\n",
       "    src=\"iframe_figures/figure_26.html\"\n",
       "    frameborder=\"0\"\n",
       "    allowfullscreen\n",
       "></iframe>\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "topic_model.visualize_topics().show(renderer='iframe_connected')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "ab85f6cd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<iframe\n",
       "    scrolling=\"no\"\n",
       "    width=\"1220px\"\n",
       "    height=\"770\"\n",
       "    src=\"iframe_figures/figure_27.html\"\n",
       "    frameborder=\"0\"\n",
       "    allowfullscreen\n",
       "></iframe>\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "reduced_embeddings = UMAP(n_neighbors=10, n_components=2, min_dist=0.0, metric='cosine').fit_transform(embeddings)\n",
    "topic_model.visualize_documents(docs, reduced_embeddings=reduced_embeddings).show(renderer='iframe_connected')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24baae50",
   "metadata": {},
   "source": [
    "Update the topic labels."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "4840c414",
   "metadata": {},
   "outputs": [],
   "source": [
    "topic_labels = topic_model.generate_topic_labels(nr_words=3,\n",
    "                                topic_prefix=True,\n",
    "                                separator=\" - \")\n",
    "\n",
    "topic_model.set_topic_labels(topic_labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d0f5b55",
   "metadata": {},
   "source": [
    "Print the results to a CSV file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "689716ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "import casanova\n",
    "\n",
    "PREDICTIONS_FILE = 'bertopic_topics_notebook.csv'\n",
    "\n",
    "results = topic_model.get_document_info(docs=docs)\n",
    "results.to_csv()\n",
    "with open(DATAFILE) as f, open(PREDICTIONS_FILE, 'w') as of:\n",
    "    enricher = casanova.enricher(f, of, add=[\"document\", \"topic\", \"name\", \"top_n_words\", \"probability\", \"representative_document\"])\n",
    "    for i, row in enumerate(enricher):\n",
    "        enricher.writerow(row=row, add=[results[\"Document\"][i], results[\"Topic\"][i], results[\"Name\"][i], results[\"Top_n_words\"][i], results[\"Probability\"][i], results[\"Representative_document\"][i]])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb76ecc1",
   "metadata": {},
   "source": [
    "# Visualize the results"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9463d5c1",
   "metadata": {},
   "source": [
    "In a dictionary, index the documents and their metadata by each document's Id."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "3e7ccda1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "\n",
    "with open(PREDICTIONS_FILE) as f:\n",
    "    reader = csv.DictReader(f)\n",
    "    for row in reader:\n",
    "        doc_index = {row[\"Id\"]:row for row in reader}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "65fbeba9",
   "metadata": {},
   "source": [
    "While parsing the matrix file, create nodes and edges and label them with data from the topic predictions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "b4efd96d",
   "metadata": {},
   "outputs": [],
   "source": [
    "MATRIX_FILE = 'defacto_covotes.csv'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "70f74086",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "IOPub data rate exceeded.\n",
      "The notebook server will temporarily stop sending output\n",
      "to the client in order to avoid crashing it.\n",
      "To change this limit, set the config variable\n",
      "`--NotebookApp.iopub_data_rate_limit`.\n",
      "\n",
      "Current values:\n",
      "NotebookApp.iopub_data_rate_limit=1000000.0 (bytes/sec)\n",
      "NotebookApp.rate_limit_window=3.0 (secs)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import networkx as nx\n",
    "from statistics import mean\n",
    "import casanova\n",
    "\n",
    "with open(MATRIX_FILE) as f:\n",
    "    reader = casanova.reader(f)\n",
    "    pid1_pos = reader.headers['pid1']\n",
    "    pid2_pos = reader.headers['pid2']\n",
    "    vote1_pos = reader.headers['vote1']\n",
    "    vote2_pos = reader.headers['vote2']\n",
    "    count_pos = reader.headers['count']\n",
    "    \n",
    "    G = nx.Graph()\n",
    "    for row in reader:\n",
    "        pid1 = row[pid1_pos]\n",
    "        pid2 = row[pid2_pos]\n",
    "\n",
    "        # If the matrix refers to documents not in the original data, skip \n",
    "        if not doc_index.get(pid1) or not doc_index.get(pid2):\n",
    "            continue\n",
    "\n",
    "        # Base an edge's weight on the support that two related propositions received\n",
    "        average_nb_votes = mean([int(doc_index[pid1][\"Nb de votes\"]), int(doc_index[pid2][\"Nb de votes\"])])\n",
    "        weight = int(row[count_pos])/average_nb_votes\n",
    "\n",
    "        # Unless already added to the Graph, add both nodes in the matrix row and create an edge between them\n",
    "        print(doc_index[pid1])\n",
    "        if not G.has_node(pid1) and not str(doc_index[pid1][\"topic\"]) == \"-1\":\n",
    "            G.add_node(pid1, label=doc_index[pid1][\"Proposition\"], **doc_index[pid1])\n",
    "\n",
    "        if not G.has_node(pid2) and not str(doc_index[pid2][\"topic\"]) == \"-1\":\n",
    "            G.add_node(pid2, label=doc_index[pid2][\"Proposition\"], **doc_index[pid2])\n",
    "\n",
    "        if row[vote1_pos] == row[vote2_pos]:  # To-do: test if we can try all 3 types of cases\n",
    "            G.add_edge(pid1, pid2, weight=weight)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "4bcd266e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pelote\n",
    "\n",
    "GEFX_FILE = 'sparsification.gexf'\n",
    "\n",
    "H = pelote.multiscale_backbone(G, alpha=0.05)\n",
    "nx.write_gexf(H, GEFX_FILE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "0319d78c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import networkx as nx\n",
    "from ipysigma import Sigma"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "c5f3e7af",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importing a gexf graph\n",
    "g = nx.read_gexf(GEFX_FILE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "ad6bc506",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0a33512526f54b0da6ce2e7b1bf3f415",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Sigma(nx.Graph with 1,722 nodes and 30,552 edges)"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Displaying the graph with a size mapped on degree and\n",
    "# a color mapped on a categorical attribute of the nodes\n",
    "Sigma(g, node_size=g.degree, node_color='topic')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54e0445d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
